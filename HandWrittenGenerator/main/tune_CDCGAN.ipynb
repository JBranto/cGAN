{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "currentPath = os.getcwd().split('/')\n",
    "\n",
    "indexOf = currentPath.index('workspace')\n",
    "rootPath = '/'.join(currentPath[:indexOf+1])+'/CDCGAN'\n",
    "os.chdir(rootPath)\n",
    "root = os.getcwd()\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import packages.arquitectures.CDCGAN as GanMannager\n",
    "from packages.dataHandlers.datasetMannager import datasetMannager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "explorationName = 'Letters_Tun02'\n",
    "gan = False\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        run.name = f\"Run-{run.id}\"\n",
    "        print(run.id)\n",
    "        config = run.config\n",
    "\n",
    "        Z_SIZE = config.size_z\n",
    "        IMG_SIZE = 64\n",
    "        IMG_CHANNELS = 1\n",
    "        BATCH_SIZE = config.batch_size\n",
    "\n",
    "        DATASET_NAME = 'LETTERS' #run.project\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(IMG_SIZE),\n",
    "            transforms.Grayscale(num_output_channels=IMG_CHANNELS),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "        dataMannager = datasetMannager(transform, BATCH_SIZE, DATASET_NAME)\n",
    "        data_mod = dataMannager.getDataModule()\n",
    "        DATALOADER = data_mod.getTrainDataLoader()\n",
    "        UNIQUE_LABELS = data_mod.getUniqueLabels()\n",
    "        LABELS_COUNT = UNIQUE_LABELS.shape[0]\n",
    "        NUM_EPOCH = config.epochs\n",
    "        KSTEPS  = config.steps\n",
    "        # print(UNIQUE_LABELS.shape[0])\n",
    "\n",
    "        gan = GanMannager.CDCGAN(isDebugMode=False,root=root)\n",
    "        gan.setDataLoader(DATALOADER, DATASET_NAME)\n",
    "        gan.setImageParams(BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, UNIQUE_LABELS, LABELS_COUNT)\n",
    "        gan.setFixedSpace(Z_SIZE, LABELS_COUNT*LABELS_COUNT, LABELS_COUNT)\n",
    "        gan.setupModels(config)\n",
    "\n",
    "        step = 0\n",
    "        D_losses, G_losses = [], []\n",
    "        Dx_values, DGz_values = [], []\n",
    "        \n",
    "        for epoch in range(NUM_EPOCH):\n",
    "            dis_total_loss, dis_real_loss = 0, 0\n",
    "            gen_loss, dis_z_loss = 0, 0\n",
    "            epoch_D_losses, epoch_G_losses = [], []\n",
    "            epoch_Dx, epoch_DGz = [], []\n",
    "\n",
    "            for real_image, real_label in DATALOADER:\n",
    "                step += 1\n",
    "                \n",
    "                dis_total_loss, dis_real_loss = gan.trainStepDis(real_image, real_label)\n",
    "                epoch_D_losses.append(dis_total_loss)\n",
    "                epoch_Dx.append(dis_real_loss)\n",
    "                \n",
    "                # print(f\"{step}/{KSTEPS}\")\n",
    "                if step % KSTEPS == 0:\n",
    "                    dis_z_loss, gen_loss = gan.trainStepGen()\n",
    "                    epoch_DGz.append(dis_z_loss)\n",
    "                    epoch_G_losses.append(gen_loss)\n",
    "\n",
    "            else:\n",
    "                D_losses.append(sum(epoch_D_losses)/len(epoch_D_losses))\n",
    "                G_losses.append(sum(epoch_G_losses)/len(epoch_G_losses))\n",
    "                Dx_values.append(sum(epoch_Dx)/len(epoch_Dx))\n",
    "                DGz_values.append(sum(epoch_DGz)/len(epoch_DGz))\n",
    "                \n",
    "                run.log({\n",
    "                    \"epoch\" : epoch,\n",
    "                    \"d_loss\": D_losses[-1], \"g_loss\": G_losses[-1],\n",
    "                    \"acc_D(real)\" : Dx_values[-1],\"acc_D(fake)\" : DGz_values[-1]\n",
    "                })\n",
    "\n",
    "                if round(DGz_values[-1], 3) >= 1.000 or round(DGz_values[-1], 3) <= 0.000: \n",
    "                    print('Train Abort: D(fake) value is not evolving as expected')\n",
    "                    break\n",
    "                \n",
    "                print(f\" Epoch: {epoch+1}/{NUM_EPOCH} |\" \n",
    "                    + f\" D_loss = {D_losses[-1]:.3f}, G_loss = {G_losses[-1]:.3f} |\"\n",
    "                    + f\" D(real) = {Dx_values[-1]:.3f}, D(fake) = {DGz_values[-1]:.3f}\")\n",
    "\n",
    "                # if(epoch % 5 == 0): \n",
    "                gan.createSamplesTable(LABELS_COUNT, epoch, NUM_EPOCH, explorationName, run.id) #OR LABELS_COUNT\n",
    "                \n",
    "        loss_plot = wandb.plot.line_series(\n",
    "            title=\"GAN loss during training\",\n",
    "            keys=[\"D loss\", \"G loss\"],\n",
    "            xs=list(range(0, NUM_EPOCH)), \n",
    "            ys=[D_losses, [item.cpu().detach().tolist() for item in G_losses ]],\n",
    "            xname=\"num epochs\"\n",
    "        )\n",
    "        acc_plot = wandb.plot.line_series(\n",
    "            title=\"GAN Acc during training\",\n",
    "            keys=[\"D(x) \", \"D(G(z))\"],\n",
    "            xs=list(range(0, NUM_EPOCH)), \n",
    "            ys=[Dx_values,  DGz_values],\n",
    "            xname=\"num epochs\"\n",
    "        )\n",
    "\n",
    "        run.log({\n",
    "            \"Loss plot\": loss_plot,\n",
    "            \"Acc plot\": acc_plot\n",
    "        })\n",
    "\n",
    "        if epoch+1== NUM_EPOCH:\n",
    "            gan.saveModel(explorationName, run.id, f\"model_run_{run.id}\")\n",
    "\n",
    "        run.finish()\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'value':'adam' \n",
    "        #'values': ['adam', 'sgd']\n",
    "    },\n",
    "    'epochs': { \n",
    "        'value': 400 \n",
    "    },\n",
    "    'steps': {\n",
    "        'value': 1\n",
    "        # 'values': [4,6,8,10,12,14,16,18,20]\n",
    "    },\n",
    "    'size_z': {\n",
    "        'value': 512\n",
    "        # 'values': [100, 120, 140, 160, 180, 200]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 128 with evenly-distributed logarithms\n",
    "        # 'distribution': 'q_log_uniform_values',\n",
    "        # 'q': 8, 'min': 64, 'max': 128,\n",
    "        'value': 128\n",
    "    },\n",
    "    'learning_rate': { # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        # 'min': 0.00078, 'max': 0.00099\n",
    "        'min': 0.00001, 'max': 0.001\n",
    "        # 'value': 0.0001\n",
    "        # 'min': 0.001, 'max': 0.003\n",
    "    },\n",
    "    'betas_min': {\n",
    "        # 'value': 0.9\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.1, 'max': 0.9\n",
    "        # 'min': 0.5, 'max': 0.9\n",
    "    },\n",
    "    'betas_max': {\n",
    "        # 'value': 0.999\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.1, 'max': 0.999\n",
    "    },\n",
    "    # 'momentun':{ # only when optim is sgd\n",
    "    #     'value': 0.9\n",
    "    # }\n",
    "}\n",
    "\n",
    "sweep_config = {'method': 'random'} ##\n",
    "sweep_config['metric'] = {\n",
    "    'name': 'g_loss',\n",
    "    'goal': 'minimize'\n",
    "}\n",
    "sweep_config['parameters'] = parameters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=explorationName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n"
     ]
    }
   ],
   "source": [
    "# experiments = 10\n",
    "\n",
    "# for i in range(experiments):\n",
    "wandb.agent(sweep_id, train, count=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
